<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>In-Memory Computing: Bringing Memory and Processing Together</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: #fafafa;
            color: #333;
        }

        header {
            background: url('header-bg.jpg') no-repeat center center;
            background-size: cover;
            color: white;
            padding: 60px 20px;
            text-align: center;
            position: relative;
        }

        /* Add a dark overlay for better text visibility */
        header::before {
            content: "";
            position: absolute;
            top: 0; left: 0;
            width: 100%; height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 0;
        }

        header h1 {
            position: relative;
            z-index: 1;
            font-size: 2.5em;
            margin: 0;
        }

        main {
            display: flex;
            max-width: 1200px;
            margin: auto;
            padding: 20px;
            gap: 20px;
        }

        article {
            flex: 3;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 10px rgba(0,0,0,0.05);
        }

        aside {
            flex: 1;
            background: #f0f0f0;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 0 5px rgba(0,0,0,0.05);
        }

        h1, h2, h3 {
            color: #1a1a1a;
        }

        strong {
            color: #000;
        }

        ul, ol {
            margin: 10px 0;
            padding-left: 20px;
        }

        li {
            margin-bottom: 8px;
        }

        hr {
            margin: 25px 0;
            border: none;
            border-top: 1px solid #ddd;
        }

        figure {
            margin: 0 0 20px 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            border-radius: 5px;
        }

        figcaption {
            font-size: 0.9em;
            color: #555;
        }
    </style>
</head>
<body>

<header>
    <h1>Neuromorphic Computing Blog</h1>
</header>

<main>
    <!-- Main Article Section -->
    <article>
        <h1>In-Memory Computing: Bringing Memory and Processing Together</h1>

        <p>One of the biggest bottlenecks in today‚Äôs computers isn‚Äôt how fast they can calculate ‚Äî it‚Äôs how often they have to <strong>move data</strong> between the processor and the memory.
        This constant back-and-forth, known as the <strong>von Neumann bottleneck</strong>, wastes time and energy.</p>

        <p><strong>In-memory computing</strong> aims to solve this by doing something radically different:<br>
        üí° <strong>Perform the computation directly where the data is stored.</strong></p>

        <hr>

        <h3>Why Moving Data is a Problem</h3>
        <p>In a traditional computer:</p>
        <ol>
            <li>Data is stored in <strong>memory</strong> (RAM, cache, or storage).</li>
            <li>The processor <strong>fetches</strong> the data from memory.</li>
            <li>The processor <strong>processes</strong> the data.</li>
            <li>Results are <strong>sent back</strong> to memory.</li>
        </ol>
        <p>For small tasks, this works fine. But modern AI workloads deal with billions of data points. Fetching and writing data over and over wastes huge amounts of energy.</p>

        <hr>

        <h3>How In-Memory Computing Works</h3>
        <p>Instead of separating memory and logic:</p>
        <ul>
            <li>Memory cells (like <strong>memristors</strong>, phase-change memory, or MRAM) are designed to <strong>store data and also perform operations</strong>.</li>
            <li>Common tasks, like multiplication and addition for neural networks, can happen <strong>inside</strong> the memory array.</li>
            <li>This reduces data movement, making computation <strong>faster and far more energy-efficient</strong>.</li>
        </ul>

        <hr>

        <h3>Why It‚Äôs Important for Neuromorphic Computing</h3>
        <p>Neuromorphic chips mimic the brain‚Äôs neuron‚Äìsynapse structure. In biology:</p>
        <ul>
            <li><strong>Neurons</strong> process signals.</li>
            <li><strong>Synapses</strong> both store connection strength and influence signal transmission.</li>
        </ul>
        <p>In-memory computing is the hardware counterpart to that idea ‚Äî the ‚Äúsynapse‚Äù stores and processes at the same location. This:</p>
        <ul>
            <li>Reduces latency.</li>
            <li>Cuts power consumption.</li>
            <li>Enables real-time processing in AI and edge devices.</li>
        </ul>

        <hr>

        <h3>Real-World Examples</h3>
        <ul>
            <li><strong>IBM</strong> is exploring in-memory computing for AI accelerators.</li>
            <li><strong>Intel‚Äôs Loihi</strong> uses concepts similar to in-memory processing for neuromorphic tasks.</li>
            <li>Research labs are building <strong>memristor-based arrays</strong> that can perform matrix multiplication inside memory.</li>
        </ul>



                <hr>

        <h3>2D Materials in In-Memory Computing</h3>
        <p>
            Recent research has explored the use of <strong>two-dimensional (2D) materials</strong>, such as graphene, transition metal dichalcogenides (TMDs), and hexagonal boron nitride, in developing next-generation in-memory computing devices.
            These materials offer exceptional electrical, optical, and mechanical properties, enabling ultra-thin, high-density, and energy-efficient memory arrays.
            Their atomic-scale thickness allows for precise control over charge transport, making them promising candidates for scalable neuromorphic hardware.
        </p>
        <p>
            Studies have evaluated various 2D memory technologies for performance, endurance, and compatibility with in-memory processing architectures, showing strong potential for both AI acceleration and edge computing applications.
        </p>

        <p><small>
            Reference: Sun, Yibo; Wang, Shuiyuan; Zhang, Qiran; Zhou, Peng. 
            <em>Evaluation of different 2D memory technologies for in-memory computing</em>, 
            Device, 2(12), 2024. Elsevier.
        </small></p>

        
    </article>

    <!-- Sidebar for Figures -->
    <aside>
    <figure>
        <a href="https://engineering.purdue.edu/ECE/News/2019/boosting-the-brains-of-computers-with-less-energy" target="_blank">
            <img src="im/von_Neumann_bottleneck.png" alt="Von Neumann vs In-Memory Computing">
        </a>
        <figcaption>von Neumann bottleneck</figcaption>
    </figure>

    <figure>
        <a href="https://www.researchgate.net/publication/360786529_Tolerating_Noise_Effects_in_Processing-in-Memory_Systems_for_Neural_Networks_A_Hardware-Software_Codesign_Perspective/figures" target="_blank">
            <img src="im/Comparison-between-the-von-Neumann-architecture-and-in-memory-computing-Reproduced-with_W640.jpg" alt="Von Neumann vs In-Memory Computing">
        </a>
        <figcaption>von Neumann vs in-memory computing</figcaption>
    </figure>

    <figure>
        <a href="https://www.embedded.com/reram-gains-interest-for-in-computing-memory/" target="_blank">
            <img src="im/IEDMroundup-image1.png" alt="Memristor Array">
        </a>
        <figcaption>
            Traditional vs. in-memory architecture.  
            <br>
            Source: Li et al., <em>J. Phys. D: Appl. Phys.</em>, 2018.
        </figcaption>
    </figure>

    <figure>
        <img src="im/I.png" alt="Von Neumann vs In-Memory Computing">
        <figcaption>
            Memristor-based in-memory computing array.  
            <br>
            <strong>
                <a href="https://doi.org/10.1088/1361-6463/aae223" target="_blank">
                    Review of memristor devices in neuromorphic computing: materials sciences and device challenges
                </a>
            </strong><br>
            Li, Yibo; Wang, Zhongrui; Midya, Rivu; Xia, Qiangfei; Yang, J. Joshua. (2018).  
            <em>Journal of Physics D: Applied Physics</em>, 51(50), 503002.
        </figcaption>
    </figure>
</aside>

</main>

</body>
</html>
